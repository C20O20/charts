apiVersion: {{ printf "%s/v1" (.Values.prometheusOperator.crdApiGroup | default "monitoring.coreos.com") }}
kind: PrometheusRule
metadata:
  name: {{ printf "%s-%s" (include "prometheus-operator.fullname" .) "alcide-server.rules" | trunc 63 | trimSuffix "-" }}
  labels:
    app: {{ template "prometheus-operator.name" . }}
{{ include "prometheus-operator.labels" . | indent 4 }}
{{- if .Values.defaultRules.labels }}
{{ toYaml .Values.defaultRules.labels | indent 4 }}
{{- end }}
{{- if .Values.defaultRules.annotations }}
  annotations:
{{ toYaml .Values.defaultRules.annotations | indent 4 }}
{{- end }}
spec:
  groups:
    - name: alcide-server.rules
      rules:
      {{/*- alert: InventoryProcessorErrorsCount*/}}
        {{/*expr: 'increase({__name__=~".+InventoryEventsProcessorImpl_errors"}[5m]) > 0'*/}}
        {{/*labels:*/}}
            {{/*severity: error*/}}
            {{/*component: server*/}}
        {{/*annotations:*/}}
            {{/*description: "Inventory Processor {{`{{ $labels.source }}`}}  has ({{`{{ $value }}`}}) errors - {{`{{ $labels.__name__ }}`}}"*/}}
            {{/*summary: Inventory Processor errors count*/}}
            {{/*env: "{{`{{ $labels.environment }}`}}"*/}}

      {{/*- alert: InventoryProcessorNoEvents*/}}
        {{/*expr: '{__name__=~".+InventoryEventsProcessorImpl_eventsRate", type="rate5"} == 0'*/}}
        {{/*for: 10m*/}}
        {{/*labels:*/}}
            {{/*severity: warning*/}}
            {{/*component: server*/}}
        {{/*annotations:*/}}
            {{/*description: "Inventory Processor {{`{{ $labels.source }}`}}  processed no event in last 10 minutes - {{`{{ $labels.__name__ }}`}}"*/}}
            {{/*summary: Inventory Processor no events*/}}
            {{/*env: "{{`{{ $labels.environment }}`}}"*/}}

      {{/*- alert: ActivityProcessorErrorsCount*/}}
        {{/*expr: 'increase({__name__=~".+ActivityEventsProcessorImpl_errors"}[5m]) > 0'*/}}
        {{/*labels:*/}}
            {{/*severity: error*/}}
            {{/*component: server*/}}
        {{/*annotations:*/}}
            {{/*description: "Activity Processor {{`{{ $labels.source }}`}}  has ({{`{{ $value }}`}}) errors - {{`{{ $labels.__name__ }}`}}"*/}}
            {{/*summary: ActivityProcessor errors count*/}}
            {{/*env: "{{`{{ $labels.environment }}`}}"*/}}

      {{/*- alert: ActivityProcessorNoEvents*/}}
        {{/*expr: '{__name__=~".+ActivityEventsProcessorImpl_eventsRate", type="rate5"} == 0'*/}}
        {{/*for: 10m*/}}
        {{/*labels:*/}}
            {{/*severity: warning*/}}
            {{/*component: server*/}}
        {{/*annotations:*/}}
            {{/*description: "Activity Processor {{`{{ $labels.source }}`}}  processed no event in last 10 minutes - {{`{{ $labels.__name__ }}`}}"*/}}
            {{/*summary: Activity Processor - no events*/}}
            {{/*env: "{{`{{ $labels.environment }}`}}"*/}}

      {{/*- alert: AlertProcessorErrorsCount*/}}
        {{/*expr: 'increase({__name__=~".+AlertLogEventsProcessorImpl_errors"}[5m]) > 0'*/}}
        {{/*labels:*/}}
            {{/*severity: error*/}}
            {{/*component: server*/}}
        {{/*annotations:*/}}
            {{/*description: "Alert Processor {{`{{ $labels.source }}`}}  has ({{`{{ $value }}`}}) errors - {{`{{ $labels.__name__ }}`}}"*/}}
            {{/*summary: Alert Processor errors count*/}}
            {{/*env: "{{`{{ $labels.environment }}`}}"*/}}

      {{/*- alert: AlertLevelProcessorErrorsCount*/}}
        {{/*expr: 'increase({__name__=~".+EntitiesAlertLevelEventsProcessorImpl_errors"}[5m]) > 0'*/}}
        {{/*labels:*/}}
            {{/*severity: error*/}}
            {{/*component: server*/}}
        {{/*annotations:*/}}
            {{/*description: "Alert Level Processor {{`{{ $labels.source }}`}}  has ({{`{{ $value }}`}}) errors - {{`{{ $labels.__name__ }}`}}"*/}}
            {{/*summary: Alert Level Processor errors count*/}}
            {{/*env: "{{`{{ $labels.environment }}`}}"*/}}

      {{/*- alert: SecurityFeaturesErrorsCount*/}}
        {{/*expr: 'increase({__name__=~".+SecurityFeaturesProcessorImpl_errors"}[5m]) > 0'*/}}
        {{/*labels:*/}}
            {{/*severity: error*/}}
            {{/*component: server*/}}
        {{/*annotations:*/}}
            {{/*description: "Security Features Processor {{`{{ $labels.source }}`}}  has ({{`{{ $value }}`}}) errors - {{`{{ $labels.__name__ }}`}}"*/}}
            {{/*summary: Security Features Processor errors count*/}}
            {{/*env: "{{`{{ $labels.environment }}`}}"*/}}

      {{/*- alert: SecurityFeaturesProcessorNoEvents*/}}
        {{/*expr: '{__name__=~".+SecurityFeaturesProcessorImpl_eventsRate", type="rate5"} == 0'*/}}
        {{/*for: 10m*/}}
        {{/*labels:*/}}
            {{/*severity: warning*/}}
            {{/*component: server*/}}
        {{/*annotations:*/}}
            {{/*description: "Security Features Processor {{`{{ $labels.source }}`}}  processed no event in last 10 minutes - {{`{{ $labels.__name__ }}`}}"*/}}
            {{/*summary: Security Features Processor - no events*/}}
            {{/*env: "{{`{{ $labels.environment }}`}}"*/}}

      {{/*- alert: GraphQLErrorsCount*/}}
        {{/*expr: 'increase({__name__=~".+GraphQLExecutorImpl_errors"}[5m]) > 0'*/}}
        {{/*labels:*/}}
            {{/*severity: error*/}}
            {{/*component: server*/}}
        {{/*annotations:*/}}
            {{/*description: "GraphQL {{`{{ $labels.source }}`}}  has ({{`{{ $value }}`}}) errors - {{`{{ $labels.__name__ }}`}}"*/}}
            {{/*summary: GraphQL errors count*/}}
            {{/*env: "{{`{{ $labels.environment }}`}}"*/}}

      {{/*- alert: GraphQLMaxLatency*/}}
        {{/*expr: '{__name__=~".+GraphQLExecutorImpl_requestsRate", type="max"} > 10000'*/}}
        {{/*labels:*/}}
            {{/*severity: warning*/}}
            {{/*component: server*/}}
        {{/*annotations:*/}}
            {{/*description: "GraphQL {{`{{ $labels.source }}`}}  has recent request processing latency of ({{`{{ $value }}`}}) MSec - {{`{{ $labels.__name__ }}`}}"*/}}
            {{/*summary: GraphQL request latency*/}}
            {{/*env: "{{`{{ $labels.environment }}`}}"*/}}

      - alert: CassandraErrorsCount
        expr: 'increase({__name__=~".+DbStoreImpl_errors"}[5m]) > 0'
        labels:
            severity: error
            component: server
        annotations:
            description: "Cassandra {{`{{ $labels.source }}`}} has ({{`{{ $value }}`}}) errors - {{`{{ $labels.__name__ }}`}}"
            summary: Cassandra errors count
            env: "{{`{{ $labels.environment }}`}}"

      {{/*- alert: IgniteErrorsRateIncrease*/}}
        {{/*expr: 'increase({__name__=~".+ignite.+Appender_error", type="count"}[5m]) > 0'*/}}
        {{/*labels:*/}}
            {{/*severity: error*/}}
            {{/*component: server*/}}
        {{/*annotations:*/}}
            {{/*description: "Ignite {{`{{ $labels.source }}`}}  has ({{`{{ $value }}`}})-per-second recent errors rate - {{`{{ $labels.__name__ }}`}}"*/}}
            {{/*summary: Ignite errors rate*/}}
            {{/*env: "{{`{{ $labels.environment }}`}}"*/}}

      {{/*- alert: IgniteNoLogs*/}}
        {{/*expr: '{__name__=~".+ignite.+Appender_info", type="rate5"} == 0'*/}}
        {{/*for: 10m*/}}
        {{/*labels:*/}}
            {{/*severity: warning*/}}
            {{/*component: server*/}}
        {{/*annotations:*/}}
            {{/*description: "Ignite {{`{{ $labels.source }}`}}  is inactive in last 5 minutes - {{`{{ $labels.__name__ }}`}}"*/}}
            {{/*summary: Ignite inactive*/}}
            {{/*env: "{{`{{ $labels.environment }}`}}"*/}}

      {{/*- alert: JVMMemoryHeapUsage*/}}
        {{/*expr: '{__name__=~".+JVM_Memory_heap_usage"} > 0.95'*/}}
        {{/*labels:*/}}
            {{/*severity: warning*/}}
            {{/*component: server*/}}
        {{/*annotations:*/}}
            {{/*description: "Process {{`{{ $labels.source }}`}}  has memory heap usage of ({{`{{ $value }}`}}) - {{`{{ $labels.__name__ }}`}}"*/}}
            {{/*summary: Memory heap almost exhausted*/}}
            {{/*env: "{{`{{ $labels.environment }}`}}"*/}}

      {{/*- alert: LoggedErrors*/}}
        {{/*expr: 'increase({__name__=~".+_Appender_error"}[5m]) > 3'*/}}
        {{/*labels:*/}}
            {{/*severity: warning*/}}
            {{/*component: server*/}}
        {{/*annotations:*/}}
            {{/*description: "{{`{{ $labels.source }}`}}  has ({{`{{ $value }}`}}) logged errors in recent minutes - {{`{{ $labels.__name__ }}`}}"*/}}
            {{/*summary: Logged errors count*/}}
            {{/*env: "{{`{{ $labels.environment }}`}}"*/}}

      - alert: KafkaInventoryLag
        expr: 'kafka_consumergroup_lag{topic="inventory",consumergroup!~".*e2e.*"} > 2000'
        for: 10m
        labels:
            severity: critical
            component: server
        annotations:
            description: "kafka lag in topic:{{`{{ $labels.topic }}`}} partition:{{`{{ $labels.partition }}`}} has excedded lag threshold of 2000"
            summary: lag was discovered in topic:{{`{{ $labels.topic }}`}} at partition:{{`{{ $labels.partition }}`}}

      - alert: KafkaComponentsActivityLag
        expr: 'kafka_consumergroup_lag{topic="components-activity",consumergroup!~".*e2e.*"} > 2000'
        for: 10m
        labels:
            severity: critical
            component: server
        annotations:
            description: "kafka lag in topic:{{`{{ $labels.topic }}`}} partition:{{`{{ $labels.partition }}`}} has excedded lag threshold of 2000"
            summary: lag was discovered in topic:{{`{{ $labels.topic }}`}} at partition:{{`{{ $labels.partition }}`}}

      - alert: KafkaNetActivityLag
        expr: 'kafka_consumergroup_lag{topic="netactivity",consumergroup!~".*e2e.*"} > 2000'
        for: 10m
        labels:
            severity: critical
            component: server
        annotations:
            description: "kafka lag in topic:{{`{{ $labels.topic }}`}} partition:{{`{{ $labels.partition }}`}} has excedded lag threshold of 2000"
            summary: lag was discovered in topic:{{`{{ $labels.topic }}`}} at partition:{{`{{ $labels.partition }}`}}

      - alert: KafkaComponentsActivityLagRate
        expr: 'irate(kafka_consumergroup_lag{topic="ComponentsActivity",consumergroup!~".*e2e.*"}[5m]) > 20'
        labels:
            severity: warning
            component: server
        annotations:
            description: "kafka consumer group lag in topic:{{`{{ $labels.topic }}`}} at partition:{{`{{ $labels.partition }}`}} has increased by more than 20 message in the last 5 minutes"
            summary: consumer group increasing lag was discovered in topic:{{`{{ $labels.topic }}`}} at partition:{{`{{ $labels.partition }}`}}

      - alert: KafkaNetActivityLagRate
        expr: 'irate(kafka_consumergroup_lag{topic="NetActivity",consumergroup!~".*e2e.*"}[5m]) > 20'
        labels:
            severity: warning
            component: server
        annotations:
            description: "kafka consumer group lag in topic:{{`{{ $labels.topic }}`}} at partition:{{`{{ $labels.partition }}`}} has increased by more than 20 message in the last 5 minutes"
            summary: consumer group increasing lag was discovered in topic:{{`{{ $labels.topic }}`}} at partition:{{`{{ $labels.partition }}`}}

      - alert: KafkaComponentsActivityLagRate
        expr: 'irate(kafka_consumergroup_lag{topic="ComponentsActivity",consumergroup!~".*e2e.*"}[5m]) > 20'
        labels:
            severity: warning
            component: server
        annotations:
            description: "kafka consumer group lag in topic:{{`{{ $labels.topic }}`}} at partition:{{`{{ $labels.partition }}`}} has increased by more than 20 message in the last 5 minutes"
            summary: consumer group increasing lag was discovered in topic:{{`{{ $labels.topic }}`}} at partition:{{`{{ $labels.partition }}`}}

      - alert: KafkaUnderReplacatedPartitions
        expr: 'kafka_cluster_partition_underreplicated{topic!~"__consumer_offsets"} > 0'
        labels:
            severity: warning
            component: server
        annotations:
            description: "kafka has lost a member in ISR at topic:{{`{{ $labels.topic }}`}} partition:{{`{{ $labels.partition }}`}}"
            summary: "Broker was removed from ISR at topic:{{`{{ $labels.topic }}`}} partition:{{`{{ $labels.partition }}`}}"

      - alert: MongoDBQueuedWriteRequests
        expr: 'increase(mongodb_mongod_global_lock_client{type="writer",namespace="alcide"}[10m]) > 100'
        labels:
            severity: critical
            component: server
        annotations:
            description: "{{`{{ $labels.namespace }}`}}/{{`{{ $labels.pod }}`}} write queue has increased by {{`{{ $value }}`}} within the last 10 minutes"

      - alert: MongoDBQueuedReadRequests
        expr: 'increase(mongodb_mongod_global_lock_client{type="reader",namespace="alcide"}[10m]) > 100'
        labels:
            severity: critical
            component: server
        annotations:
            description: "{{`{{ $labels.namespace }}`}}/{{`{{ $labels.pod }}`}} read queue has increased by {{`{{ $value }}`}} within the last 10 minutes"

      - alert: MongoDBReplicationLagWarning
        expr: '(max(mongodb_mongod_replset_member_optime_date{state="PRIMARY", set="$replset"}) - min(mongodb_mongod_replset_member_optime_date{state="SECONDARY"})) or vector(0) > 60'
        labels:
            severity: warning
            component: server
        annotations:
            summary: "Current MongoDB replication lag is {{`{{ $value }}`}} seconds"
            description: "A high replication lag can be due to: Networking issue between the primary and secondary making nodes unreachable: check members.state to spot unreachable nodesA secondary node applying data slower than the primary. Insufficient write capacity in which case you should add more shards: check queued write requests. Slow operations on the primary node blocking replication. You can spot slow queries with the db.getProfilingStatus() command or through the Visual Query Profiler in MongoDB Ops Manager if you are using it: level 1 corresponds to slow operations (taking longer than the threshold defined by the operationProfiling.slowOpThresholdMs parameter set to 100 ms by default). It can be due to heavy write operations on the primary node or an under-provisioned secondary. You can prevent the latter by scaling up the secondary to match the primary capacity. You can use “majority” write concern to make sure writes are not getting ahead of replication."

      - alert: MongoDBReplicationLagCritical
        expr: '(max(mongodb_mongod_replset_member_optime_date{state="PRIMARY", set="$replset"}) - min(mongodb_mongod_replset_member_optime_date{state="SECONDARY"})) or vector(0) > 240'
        labels:
            severity: critical
            component: server
        annotations:
            summary: "Current MongoDB replication lag is {{`{{ $value }}`}} seconds"
            description: "A high replication lag can be due to: Networking issue between the primary and secondary making nodes unreachable: check members.state to spot unreachable nodes. A secondary node applying data slower than the primary. Insufficient write capacity in which case you should add more shards: check queued write requests. Slow operations on the primary node blocking replication. You can spot slow queries with the db.getProfilingStatus() command or through the Visual Query Profiler in MongoDB Ops Manager if you are using it: level 1 corresponds to slow operations (taking longer than the threshold defined by the operationProfiling.slowOpThresholdMs parameter set to 100 ms by default). It can be due to heavy write operations on the primary node or an under-provisioned secondary. You can prevent the latter by scaling up the secondary to match the primary capacity. You can use “majority” write concern to make sure writes are not getting ahead of replication."

      - alert: MongoDBReplicaInRecoveryState
        expr: 'mongodb_mongod_replset_member_state == 3'
        labels:
            severity: warning
            component: server
        annotations:
            summary: "{{`{{ $labels.namespace }}`}}/{{`{{ $labels.pod }}`}} is in RECOVERY mode."
            description: "{{`{{ $labels.namespace }}`}}/{{`{{ $labels.pod }}`}} is in RECOVERY mode. If this isn't the result of an intentional operation you should check the root cause for this status."

      - alert: MongoDBReplicaInRecoveryState
        expr: 'mongodb_mongod_replset_member_state == 3'
        labels:
            severity: warning
            component: server
        annotations:
            summary: "{{`{{ $labels.namespace }}`}}/{{`{{ $labels.pod }}`}} is in RECOVERY mode."
            description: "{{`{{ $labels.namespace }}`}}/{{`{{ $labels.pod }}`}} is in RECOVERY mode. If this isn't the result of an intentional operation you should check the root cause for this status."

      - alert: MongoDBReplicaInUnknownState
        expr: 'mongodb_mongod_replset_member_state == 6'
        labels:
            severity: critical
            component: server
        annotations:
            summary: "{{`{{ $labels.namespace }}`}}/{{`{{ $labels.pod }}`}} is in UNKNOWN state."
            description: "{{`{{ $labels.namespace }}`}}/{{`{{ $labels.pod }}`}} is in UNKNOWN state."

      - alert: MongoDBReplicaIsDown
        expr: 'mongodb_mongod_replset_member_state == 8'
        labels:
            severity: critical
            component: server
        annotations:
            summary: "{{`{{ $labels.namespace }}`}}/{{`{{ $labels.pod }}`}} is in CRITICAL state."
            description: "{{`{{ $labels.namespace }}`}}/{{`{{ $labels.pod }}`}} is in CRITICAL state."

      - alert: MongoDBReplicaWasRemoved
        expr: 'mongodb_mongod_replset_member_state == 10'
        labels:
            severity: critical
            component: server
        annotations:
            summary: "{{`{{ $labels.namespace }}`}}/{{`{{ $labels.pod }}`}} has been removed from the replica set."
            description: "{{`{{ $labels.namespace }}`}}/{{`{{ $labels.pod }}`}} has been removed from the replica set."

      - alert: MongoDBWiredTigerConcurrentTransactionsReadAvailable
        expr: 'mongodb_mongod_wiredtiger_concurrent_transactions_available_tickets{type="read"} == 0'
        labels:
            severity: warning
            component: server
        annotations:
            summary: "{{`{{ $labels.namespace }}`}}/{{`{{ $labels.pod }}`}} is out of read tickets!"
            description: "When the number of available read or write tickets remaining reaches zero, new read or write requests will be queued until a new read or write ticket is available. The maximums of concurrent read and write operations are respectively defined by the parameters wiredTigerConcurrentReadTransactions and wiredTigerConcurrentWriteTransactions. Both are equal to 128 by default. However be careful when increasing it: if the number of simultaneous operations gets too high, you might run out of system resources (CPU in particular). Scaling horizontally by adding more shards can help to support high throughputs."

      - alert: MongoDBWiredTigerConcurrentTransactionsWriteAvailable
        expr: 'mongodb_mongod_wiredtiger_concurrent_transactions_available_tickets{type="write"} == 0'
        labels:
            severity: warning
            component: server
        annotations:
            summary: "{{`{{ $labels.namespace }}`}}/{{`{{ $labels.pod }}`}} is out of write tickets!"
            description: "When the number of available read or write tickets remaining reaches zero, new read or write requests will be queued until a new read or write ticket is available. The maximums of concurrent read and write operations are respectively defined by the parameters wiredTigerConcurrentReadTransactions and wiredTigerConcurrentWriteTransactions. Both are equal to 128 by default. However be careful when increasing it: if the number of simultaneous operations gets too high, you might run out of system resources (CPU in particular). Scaling horizontally by adding more shards can help to support high throughputs."

